{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53ea8be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 20:33:00.844829: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/home/patricknit/.mujoco/mjpro150/bin\n",
      "2022-03-16 20:33:00.844847: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/home/patricknit/anaconda3/envs/tf_gpu/lib/python3.9/site-packages/torch/cuda/__init__.py:80: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  ../c10/cuda/CUDAFunctions.cpp:112.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "from torchsummary import summary\n",
    "from collections import deque\n",
    "\n",
    "from di_dataset3 import DepthImageDataset, collate_batch\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2848f0f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c88205",
   "metadata": {},
   "source": [
    "# Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a82a7290",
   "metadata": {},
   "outputs": [],
   "source": [
    "LINUX = True\n",
    "\n",
    "latent_dim = 32\n",
    "num_epochs = 250\n",
    "batch_size = 32\n",
    "learning_rate = 1e-3\n",
    "\n",
    "save_model = True\n",
    "load_model = False\n",
    "\n",
    "if LINUX:\n",
    "    base_path = \"/home/patricknit\"\n",
    "else:\n",
    "    base_path = \"/Users/patricknitschke/Library/CloudStorage/OneDrive-NTNU/NTNU/Kybernetikk og robotikk/Master/Thesis/Code\"\n",
    "\n",
    "tfrecord_folder = base_path + \"/rl_data/tfrecord_wfiltered\"\n",
    "tfrecord_test_folder = tfrecord_folder + \"/test\"\n",
    "\n",
    "save_model_file = base_path + \"/vae_models/dronet1_32latent_MSE_weighted_filtered\" # sum and old depth gain\n",
    "load_model_file = base_path + \"/vae_models/vae_dronet_sigmoid_MSE_weighted_filtered_140.pth\"\n",
    "\n",
    "writer = SummaryWriter('runs'+save_model_file[save_model_file.rfind(\"/\"):])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f15247",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "526ce03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 20:33:01.842679: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-03-16 20:33:01.842758: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (patricknit-OptiPlex-7060): /proc/driver/nvidia/version does not exist\n",
      "2022-03-16 20:33:01.843144: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tfrecords... \t['/home/patricknit/rl_data/tfrecord_wfiltered/data43.tfrecords']\n",
      "Iterating length... \tDone: 73\n",
      "Loading tfrecords... \t['/home/patricknit/rl_data/tfrecord_wfiltered/test/data196.tfrecords']\n",
      "Iterating length... \tDone: 65\n",
      "73 65\n",
      "gotcha\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 1, 270, 480]), torch.Size([32, 1, 270, 480]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = DepthImageDataset(tfrecord_folder=tfrecord_folder, batch_size=batch_size, one_tfrecord=True) # 180 tfrecords\n",
    "test_dataset = DepthImageDataset(tfrecord_folder=tfrecord_test_folder, batch_size=batch_size, one_tfrecord=True) # 20 tfrecords\n",
    "\n",
    "len_train_dataset, len_test_dataset = len(train_dataset), len(test_dataset)\n",
    "n_training_samples = len_train_dataset * 32 # 32 samples per batch\n",
    "print(len_train_dataset, len_test_dataset)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=1, collate_fn=collate_batch)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=1, collate_fn=collate_batch)\n",
    "\n",
    "dataiter = iter(train_loader)\n",
    "images, images_filtered, *_ = dataiter.next() # image, filtered image, height, width, depth\n",
    "images.shape, images_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5eb2b73",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def imshow(image):\n",
    "    io.imshow(image.squeeze().numpy())\n",
    "    io.show()\n",
    "\n",
    "def make_grid_for_tensorboard(images_list, n_grids=2):\n",
    "    joined_images = []\n",
    "    [joined_images.extend(images[:n_grids]) for images in images_list]\n",
    "    return torchvision.utils.make_grid(joined_images, nrow=1, padding=5)\n",
    "\n",
    "def gridshow(images_list, n_grids=None):\n",
    "    for n, items in enumerate(zip(*images_list)):\n",
    "        grid = torchvision.utils.make_grid([*items], nrow=1, padding=5).permute(1, 2, 0)\n",
    "        imshow(grid)\n",
    "        if (n+1) == n_grids:\n",
    "            return\n",
    "\n",
    "#gridshow([images, images_filtered], n_grids=4)\n",
    "#imshow(make_grid_for_tensorboard([images, images_filtered]).permute(1,2,0))\n",
    "\n",
    "# num = 4\n",
    "# for image, filtered in zip(images[:num], images_filtered[:num]):\n",
    "#     imshow(image)\n",
    "#     imshow(filtered)\n",
    "    \n",
    "# image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d97d632",
   "metadata": {},
   "source": [
    "# Define Variational Autoencoder\n",
    "\n",
    "Adapted from https://github.com/microsoft/AirSim-Drone-Racing-VAE-Imitation/blob/master/racing_models/cmvae.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9e6988",
   "metadata": {},
   "source": [
    "### Dronet\n",
    "ResNet8 as encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5587be55",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dronet] Starting dronet\n",
      "[Dronet] Done with dronet\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 135, 240]             832\n",
      "            Conv2d-2          [-1, 32, 68, 120]          25,632\n",
      "       BatchNorm2d-3          [-1, 32, 68, 120]              64\n",
      "            Conv2d-4           [-1, 32, 34, 60]           9,248\n",
      "       BatchNorm2d-5           [-1, 32, 34, 60]              64\n",
      "            Conv2d-6           [-1, 32, 34, 60]           9,248\n",
      "            Conv2d-7           [-1, 32, 34, 60]           1,056\n",
      "       BatchNorm2d-8           [-1, 32, 34, 60]              64\n",
      "            Conv2d-9           [-1, 64, 17, 30]          18,496\n",
      "      BatchNorm2d-10           [-1, 64, 17, 30]             128\n",
      "           Conv2d-11           [-1, 64, 17, 30]          36,928\n",
      "           Conv2d-12           [-1, 64, 17, 30]           2,112\n",
      "      BatchNorm2d-13           [-1, 64, 17, 30]             128\n",
      "           Conv2d-14           [-1, 128, 9, 15]          73,856\n",
      "      BatchNorm2d-15           [-1, 128, 9, 15]             256\n",
      "           Conv2d-16           [-1, 128, 9, 15]         147,584\n",
      "           Conv2d-17           [-1, 128, 9, 15]           8,320\n",
      "           Linear-18                   [-1, 64]       1,105,984\n",
      "           Linear-19                   [-1, 32]           2,080\n",
      "           Linear-20                   [-1, 64]           2,112\n",
      "================================================================\n",
      "Total params: 1,444,192\n",
      "Trainable params: 1,444,192\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.49\n",
      "Forward/backward pass size (MB): 16.16\n",
      "Params size (MB): 5.51\n",
      "Estimated Total Size (MB): 22.16\n",
      "----------------------------------------------------------------\n",
      "[ImgDecoder] Starting create_model\n",
      "[ImgDecoder] Done with create_model\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1             [-1, 1, 17280]         570,240\n",
      "   ConvTranspose2d-2           [-1, 128, 9, 15]         147,584\n",
      "   ConvTranspose2d-3           [-1, 64, 17, 30]         204,864\n",
      "   ConvTranspose2d-4           [-1, 64, 34, 60]         147,520\n",
      "   ConvTranspose2d-5          [-1, 32, 68, 120]          73,760\n",
      "   ConvTranspose2d-6         [-1, 32, 135, 240]          25,632\n",
      "   ConvTranspose2d-7         [-1, 16, 270, 480]          18,448\n",
      "   ConvTranspose2d-8          [-1, 1, 270, 480]             401\n",
      "================================================================\n",
      "Total params: 1,188,449\n",
      "Trainable params: 1,188,449\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 28.22\n",
      "Params size (MB): 4.53\n",
      "Estimated Total Size (MB): 32.75\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class Dronet(nn.Module):\n",
    "    def __init__(self, input_dim, num_outputs, include_top=True):\n",
    "        super(Dronet, self).__init__()\n",
    "        self.include_top = include_top\n",
    "        \n",
    "        print('[Dronet] Starting dronet')\n",
    "\n",
    "        self.max0 = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)  # default pool_size='2', strides=2\n",
    "\n",
    "        self.bn0 = nn.BatchNorm2d(32)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        self.bn5 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.conv0 = nn.Conv2d(input_dim, 32, kernel_size=5, stride=2, padding=2)\n",
    "        self.xavier_uniform_init(self.conv0)\n",
    "        \n",
    "        self.conv0_2 = nn.Conv2d(32, 32, kernel_size=5, stride=2, padding=2)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1) # padding='same' \n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1) # padding='same' # Todo: kernel_regularizer=tf.keras.regularizers.l2(1e-4)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=1, stride=2)\n",
    "        self.xavier_uniform_init(self.conv3)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1) # padding='same' # Todo: kernel_regularizer=tf.keras.regularizers.l2(1e-4)\n",
    "        self.conv5 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1) # padding='same' # Todo: kernel_regularizer=tf.keras.regularizers.l2(1e-4)\n",
    "        self.conv6 = nn.Conv2d(32, 64, kernel_size=1, stride=2) # padding='same'\n",
    "        self.xavier_uniform_init(self.conv6)\n",
    "\n",
    "        self.conv7 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1) # padding='same' # Todo: kernel_regularizer=tf.keras.regularizers.l2(1e-4)\n",
    "        self.conv8 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1) # padding='same' # Todo: kernel_regularizer=tf.keras.regularizers.l2(1e-4)\n",
    "        self.conv9 = nn.Conv2d(64, 128, kernel_size=1, stride=2) # padding='same'\n",
    "        self.xavier_uniform_init(self.conv9)\n",
    "\n",
    "        self.dense0 = nn.Linear(9*15*128, 64) # Todo: check size\n",
    "        self.dense1 = nn.Linear(64, 32)\n",
    "        self.dense2 = nn.Linear(32, num_outputs)\n",
    "        self.dense3 = nn.Linear(9*15*128, num_outputs)\n",
    "\n",
    "        print('[Dronet] Done with dronet')\n",
    "    \n",
    "    \n",
    "    def xavier_uniform_init(self, m):\n",
    "        \"\"\"\n",
    "        Default initialisation in Keras is glorot_uniform == xavier_uniform in Pytorch\n",
    "\n",
    "        https://discuss.pytorch.org/t/crossentropyloss-expected-object-of-type-torch-longtensor/28683/6?u=ptrblck\n",
    "        https://stackoverflow.com/questions/49433936/how-to-initialize-weights-in-pytorch\n",
    "        \"\"\"\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight, gain=nn.init.calculate_gain('linear')) # gain=nn.init.calculate_gain('relu')\n",
    "            nn.init.zeros_(m.bias)\n",
    "        return m\n",
    "\n",
    "    def forward(self, img):\n",
    "        # Input\n",
    "        \n",
    "        #print(img.shape)\n",
    "        #print(f\"- Before encoding, mean: {img.mean():.3f} var: {img.var():.3f}\")\n",
    "        x1 = self.conv0(img)\n",
    "        #print(\"x1:\", x1.shape)\n",
    "        #x1, indices = self.max0(x1)\n",
    "        x1 = self.conv0_2(x1)\n",
    "        #print(\"x1:\", x1.shape)\n",
    "\n",
    "        # First residual block\n",
    "        x2 = self.bn0(x1)\n",
    "        #print(\"x2:\", x2.shape)\n",
    "        # x2 = x1\n",
    "        x2 = torch.relu(x2)\n",
    "        #print(x2.shape)\n",
    "        x2 = self.conv1(x2)\n",
    "        #print(x2.shape)\n",
    "\n",
    "        x2 = self.bn1(x2)\n",
    "        #print(x2.shape)\n",
    "        x2 = torch.relu(x2)\n",
    "        #print(x2.shape)\n",
    "        x2 = self.conv2(x2)\n",
    "        #print(x2.shape)\n",
    "        \n",
    "        \n",
    "        x1 = self.conv3(x1)\n",
    "        #print(f\"x1: {x1.shape}, x2: {x2.shape}\") # 32, 34, 60\n",
    "        x3 = x1+x2\n",
    "        #print(\"x3:\", x3.shape)\n",
    "\n",
    "        # Second residual block\n",
    "        x4 = self.bn2(x3)\n",
    "        #print(\"x4:\", x4.shape)\n",
    "        # x4 = x3\n",
    "        x4 = torch.relu(x4)\n",
    "        x4 = self.conv4(x4)\n",
    "        #print(x4.shape)\n",
    "\n",
    "        x4 = self.bn3(x4)\n",
    "        #print(x4.shape)\n",
    "        x4 = torch.relu(x4)\n",
    "        x4 = self.conv5(x4)\n",
    "        #print(x4.shape)\n",
    "        \n",
    "        x3 = self.conv6(x3)\n",
    "        #print(f\"x3: {x3.shape}, x4: {x4.shape}\")\n",
    "        x5 = x3+x4 # 34, 60, 64\n",
    "\n",
    "        # Third residual block\n",
    "        x6 = self.bn4(x5)\n",
    "        # x6 = x5\n",
    "        x6 = torch.relu(x6)\n",
    "        x6 = self.conv7(x6)\n",
    "\n",
    "        x6 = self.bn5(x6)\n",
    "        x6 = torch.relu(x6)\n",
    "        x6 = self.conv8(x6)\n",
    "\n",
    "        x5 = self.conv9(x5)\n",
    "        #print(f\"x5: {x5.shape}, x6: {x6.shape}\")\n",
    "        x7 = x5+x6 # 17, 30, 128\n",
    "\n",
    "        x = x7.view(x7.size(0), -1)\n",
    "        #print(f\"x before linear: {x.shape}\")\n",
    "        #print(f\"- After encoding, mean: {x.mean():.3f} var: {x.var():.3f}\")\n",
    "\n",
    "        if self.include_top:\n",
    "            x = torch.relu(x)\n",
    "            # x = tf.keras.layers.Dropout(0.5)(x)\n",
    "            x = self.dense0(x)\n",
    "            x = self.dense1(x)\n",
    "            #gate_pose = self.dense2(x)\n",
    "            x_enc = self.dense2(x)\n",
    "            # phi_rel = self.dense_phi_rel(x)\n",
    "            # gate_pose = tf.concat([gate_pose, phi_rel], 1)\n",
    "            #print(f\"x_enc: {x.shape}\")\n",
    "            return x_enc\n",
    "        else:\n",
    "            x = self.dense3(x)\n",
    "            #print(f\"x_enc: {x.shape}\")\n",
    "            return x\n",
    "\n",
    "dronet = Dronet(input_dim=1, num_outputs=latent_dim*2, include_top=True)\n",
    "summary(dronet, input_size=(1, 270, 480), batch_size=-1, device=device.type)\n",
    "\n",
    "\n",
    "class ImgDecoder(nn.Module):\n",
    "    def __init__(self, input_dim=100, batch=32):\n",
    "        super(ImgDecoder, self).__init__()\n",
    "        print('[ImgDecoder] Starting create_model')\n",
    "        self.dense = nn.Linear(input_dim, 9*15*128)\n",
    "        \n",
    "        self.bn0 = nn.BatchNorm2d(128)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        self.bn5 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.unpool = nn.MaxUnpool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.deconv1 = nn.ConvTranspose2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2, padding=(2,2), output_padding=(0,1), dilation=1)\n",
    "        self.deconv3 = nn.ConvTranspose2d(64, 64, kernel_size=6, stride=2, padding=(2,2), output_padding=(0,0), dilation=1)\n",
    "        self.deconv4 = nn.ConvTranspose2d(64, 32, kernel_size=6, stride=2, padding=(2,2), output_padding=(0,0), dilation=1)\n",
    "        self.deconv5 = nn.ConvTranspose2d(32, 32, kernel_size=5, stride=2, padding=(2,2), output_padding=(0,1), dilation=1)\n",
    "        self.deconv6 = nn.ConvTranspose2d(32, 16, kernel_size=6, stride=2, padding=(2,2), output_padding=(0,0))\n",
    "        self.deconv7 = nn.ConvTranspose2d(16, 1, kernel_size=5, stride=1, padding=2) # tanh activation or sigmoid\n",
    "\n",
    "        print('[ImgDecoder] Done with create_model')\n",
    "\n",
    "    def forward(self, z):\n",
    "        #print(\"z: \", z.shape)\n",
    "        x = self.dense(z)\n",
    "        #print(x.shape)\n",
    "        x = x.view(x.size(0), 128, 9, 15)\n",
    "        #print(f\"- Before deconv, mean: {x.mean():.3f} var: {x.var():.3f}\")\n",
    "        \n",
    "        x = self.deconv1(x)\n",
    "        x = torch.relu(x)\n",
    "        #print(\"after deconv1\", x.shape)\n",
    "        \n",
    "        x = self.deconv2(x)\n",
    "        x = torch.relu(x)\n",
    "        #print(\"after deconv2\", x.shape)\n",
    "        \n",
    "        x = self.deconv3(x)\n",
    "        x = torch.relu(x)\n",
    "#         print(\"after deconv3\", x.shape)\n",
    "\n",
    "        x = self.deconv4(x)\n",
    "        x = torch.relu(x)\n",
    "#         print(\"after deconv4\", x.shape)\n",
    "\n",
    "        x = self.deconv5(x)\n",
    "        #print(\"indices\", indices.shape)\n",
    "        #x = self.unpool(x, indices, output_size=(x.size(0), 1, 135, 240))\n",
    "#       print(\"after deconv5\", x.shape)\n",
    "        x = torch.relu(x)\n",
    "        #print(\"after unpool\", x.shape)\n",
    "        \n",
    "        x = self.deconv6(x)\n",
    "        x = torch.relu(x)\n",
    "#         print(\"after deconv6\", x.shape)\n",
    "\n",
    "        x = self.deconv7(x)\n",
    "        #print(f\"- After deconv 7, mean: {x.mean():.3f} var: {x.var():.3f}\")\n",
    "        x = torch.sigmoid(x)\n",
    "        #print(f\"- After sigmoid, mean: {x.mean():.3f} var: {x.var():.3f}\")\n",
    "        #print(\"after deconv7\", x.shape)\n",
    "        return x\n",
    "\n",
    "img_decoder = ImgDecoder(input_dim=latent_dim, batch=1)\n",
    "summary(img_decoder, (1,latent_dim))\n",
    "\n",
    "\n",
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super(Lambda, self).__init__()\n",
    "        self.func = func\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.func(x)\n",
    "        \n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, n_z):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # create the 3 base models:\n",
    "        self.q_img = Dronet(input_dim=1, num_outputs=n_z*2, include_top=True)\n",
    "        self.p_img = ImgDecoder(input_dim=n_z)\n",
    "        \n",
    "        # Create sampler\n",
    "        self.mean_params = Lambda(lambda x: x[:, :n_z])\n",
    "        self.logvar_params = Lambda(lambda x: x[:, n_z:])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Possible modes for reconstruction:\n",
    "        # img -> img\n",
    "\n",
    "        x = self.q_img(x)\n",
    "        \n",
    "        means = self.mean_params(x)\n",
    "        logvar = self.logvar_params(x)\n",
    "        stddev = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(stddev)\n",
    "        z = means + eps * stddev\n",
    "        #print(z.shape)\n",
    "        #print(\"OK\")\n",
    "        img_recon = self.p_img(z)\n",
    "        \n",
    "        return img_recon, means, logvar, z\n",
    "    \n",
    "    def forward_test(self, x):\n",
    "        x = self.q_img(x)\n",
    "        \n",
    "        means = self.mean_params(x)\n",
    "        logvar = self.logvar_params(x)\n",
    "        stddev = torch.exp(0.5 * logvar)\n",
    "        eps = torch.zeros_like(stddev)\n",
    "        z = means + eps * stddev\n",
    "        #print(z.shape)\n",
    "        #print(\"OK\")\n",
    "        img_recon = self.p_img(z)\n",
    "        return img_recon, means, logvar, z\n",
    "        \n",
    "    def encode(self, x):\n",
    "        x = self.q_img(x)\n",
    "        means = self.mean_params(x)\n",
    "        stddev = torch.exp(0.5 * self.stddev_params(x))\n",
    "        eps = torch.randn_like(stddev)\n",
    "        z = means + eps * stddev\n",
    "        return z, means, stddev\n",
    "\n",
    "    def decode(self, z, mode):\n",
    "        # Possible modes for reconstruction:\n",
    "        # z -> img\n",
    "        img_recon = self.p_img(z)\n",
    "        return img_recon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadac3a2",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a9d8b94",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dronet] Starting dronet\n",
      "[Dronet] Done with dronet\n",
      "[ImgDecoder] Starting create_model\n",
      "[ImgDecoder] Done with create_model\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 135, 240]             832\n",
      "            Conv2d-2          [-1, 32, 68, 120]          25,632\n",
      "       BatchNorm2d-3          [-1, 32, 68, 120]              64\n",
      "            Conv2d-4           [-1, 32, 34, 60]           9,248\n",
      "       BatchNorm2d-5           [-1, 32, 34, 60]              64\n",
      "            Conv2d-6           [-1, 32, 34, 60]           9,248\n",
      "            Conv2d-7           [-1, 32, 34, 60]           1,056\n",
      "       BatchNorm2d-8           [-1, 32, 34, 60]              64\n",
      "            Conv2d-9           [-1, 64, 17, 30]          18,496\n",
      "      BatchNorm2d-10           [-1, 64, 17, 30]             128\n",
      "           Conv2d-11           [-1, 64, 17, 30]          36,928\n",
      "           Conv2d-12           [-1, 64, 17, 30]           2,112\n",
      "      BatchNorm2d-13           [-1, 64, 17, 30]             128\n",
      "           Conv2d-14           [-1, 128, 9, 15]          73,856\n",
      "      BatchNorm2d-15           [-1, 128, 9, 15]             256\n",
      "           Conv2d-16           [-1, 128, 9, 15]         147,584\n",
      "           Conv2d-17           [-1, 128, 9, 15]           8,320\n",
      "           Linear-18                   [-1, 64]       1,105,984\n",
      "           Linear-19                   [-1, 32]           2,080\n",
      "           Linear-20                   [-1, 64]           2,112\n",
      "           Dronet-21                   [-1, 64]               0\n",
      "           Lambda-22                   [-1, 32]               0\n",
      "           Lambda-23                   [-1, 32]               0\n",
      "           Linear-24                [-1, 17280]         570,240\n",
      "  ConvTranspose2d-25           [-1, 128, 9, 15]         147,584\n",
      "  ConvTranspose2d-26           [-1, 64, 17, 30]         204,864\n",
      "  ConvTranspose2d-27           [-1, 64, 34, 60]         147,520\n",
      "  ConvTranspose2d-28          [-1, 32, 68, 120]          73,760\n",
      "  ConvTranspose2d-29         [-1, 32, 135, 240]          25,632\n",
      "  ConvTranspose2d-30         [-1, 16, 270, 480]          18,448\n",
      "  ConvTranspose2d-31          [-1, 1, 270, 480]             401\n",
      "       ImgDecoder-32          [-1, 1, 270, 480]               0\n",
      "================================================================\n",
      "Total params: 2,632,641\n",
      "Trainable params: 2,632,641\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.49\n",
      "Forward/backward pass size (MB): 45.37\n",
      "Params size (MB): 10.04\n",
      "Estimated Total Size (MB): 55.91\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "vae_model = VAE(n_z=latent_dim)\n",
    "\n",
    "\n",
    "if load_model:\n",
    "    vae_model.load_state_dict(torch.load(load_model_file))\n",
    "\n",
    "    \n",
    "optimiser = torch.optim.Adam(vae_model.parameters(), lr=learning_rate)\n",
    "summary(vae_model, (1, 270, 480))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f69f90e",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a2e92d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def depth_gain(x):\n",
    "    return 1/(x + 0.5) - 0.5\n",
    "\n",
    "x = np.arange(0, 1.001, 0.001)\n",
    "y = depth_gain(x)\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.plot(x, y)\n",
    "plt.grid()\n",
    "plt.legend([\"depth gain per pixel value\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5515c863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BCELoss(x, x_hat, mean, log_var):\n",
    "    reproduction_loss = nn.BCELoss(reduction='sum')(x_hat, x)\n",
    "    KLD      = 0.5 * torch.sum(1+ log_var - mean.pow(2) - log_var.exp())\n",
    "\n",
    "    return reproduction_loss - KLD\n",
    "\n",
    "def MSE(x, x_hat, mu, logvar):\n",
    "    #rep = (x_hat - x)**2\n",
    "    reproduction_loss = F.mse_loss(x_hat, x, reduction=\"sum\")\n",
    "    kl_divergence = 0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return reproduction_loss - kl_divergence\n",
    "\n",
    "def weighted_MSE(x, x_hat, mu, logvar):\n",
    "    reproduction_loss_ref = F.mse_loss(x_hat, x, reduction=\"sum\")\n",
    "    \n",
    "    reproduction_loss = (depth_gain(x) * torch.square(x_hat - x)).sum()\n",
    "    kl_divergence = 0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "    #print(reproduction_loss, reproduction_loss_ref)\n",
    "    return reproduction_loss - kl_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4552d13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tfrecords... \t['/home/patricknit/rl_data/tfrecord_wfiltered/data123.tfrecords']\n",
      "Iterating length... \tDone: 67\n",
      "Loading tfrecords... \t['/home/patricknit/rl_data/tfrecord_wfiltered/test/data195.tfrecords']\n",
      "Iterating length... \tDone: 66\n",
      "Training...\n",
      "gotcha\n",
      "Epoch: 1/250, Step: 5/67, Avg. train loss: 9173.913, time: 22.91, Avg. per iter 4.49, Est. time left 278.53\n",
      "Epoch: 1/250, Step: 10/67, Avg. train loss: 7067.474, time: 45.05, Avg. per iter 4.45, Est. time left 253.76\n",
      "Epoch: 1/250, Step: 15/67, Avg. train loss: 6438.315, time: 68.51, Avg. per iter 4.52, Est. time left 235.28\n",
      "Epoch: 1/250, Step: 20/67, Avg. train loss: 5764.493, time: 91.84, Avg. per iter 4.56, Est. time left 214.10\n",
      "Epoch: 1/250, Step: 25/67, Avg. train loss: 5215.973, time: 114.29, Avg. per iter 4.54, Est. time left 190.63\n"
     ]
    }
   ],
   "source": [
    "time_iteration = deque(maxlen=100)\n",
    "total_train_iterations = 0\n",
    "total_test_iterations = 0\n",
    "\n",
    "if load_model:\n",
    "    start = load_model_file.rfind(\"_\") + 1\n",
    "    end = load_model_file.rfind(\".\")\n",
    "    epoch_start = int(\"\".join([char for char in load_model_file[start:end]]))\n",
    "    num_epochs = [epoch_start, num_epochs]\n",
    "else: \n",
    "    num_epochs = num_epochs if isinstance(num_epochs, list) else [num_epochs]\n",
    "\n",
    "for epoch in range(*num_epochs):\n",
    "    # Random file each time\n",
    "    train_dataset = DepthImageDataset(tfrecord_folder=tfrecord_folder, batch_size=batch_size, one_tfrecord=True)\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=1, collate_fn=collate_batch)\n",
    "    train_iterations = len(train_loader)\n",
    "\n",
    "    test_dataset = DepthImageDataset(tfrecord_folder=tfrecord_test_folder, batch_size=batch_size, one_tfrecord=True)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=1, collate_fn=collate_batch)\n",
    "    test_iterations = len(test_loader)\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    vae_model.train()\n",
    "    running_loss = 0.0\n",
    "    since = time.time()\n",
    "    for i, (image, image_filtered, *_) in enumerate(train_loader):\n",
    "        since_iter = time.time() \n",
    "        optimiser.zero_grad()\n",
    "        \n",
    "        image = image.to(device)\n",
    "        image_filtered = image_filtered.to(device)\n",
    "\n",
    "        # VAE forward pass\n",
    "        x_hat, mu, logvar, z = vae_model(image)\n",
    "\n",
    "        # Loss\n",
    "        loss = weighted_MSE(image_filtered, x_hat, mu, logvar)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Update weights\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        time_iteration.append(time.time() - since_iter)\n",
    "        iter_time_mean = np.array(time_iteration).mean()\n",
    "    \n",
    "        # Log info\n",
    "        writer.add_scalar('training/loss', running_loss/((i+1)*batch_size), global_step=total_train_iterations)\n",
    "        if (i+1) % (5) == 0:\n",
    "            time_elapsed = time.time() - since\n",
    "            print(f\"Epoch: {epoch+1}/{num_epochs[-1]}, Step: {i+1}/{train_iterations}, Avg. train loss:\", \\\n",
    "                  f\"{running_loss/((i+1)*batch_size):.3f}, time: {time_elapsed:.2f}, Avg. per iter\", \\\n",
    "                  f\"{iter_time_mean:.2f}, Est. time left {iter_time_mean*(train_iterations - (i+1)):.2f}\")\n",
    "        if (i+1) % (5) == 0:\n",
    "            grid = make_grid_for_tensorboard([image, image_filtered, x_hat], n_grids=1)\n",
    "            writer.add_image('training/images', grid, global_step=total_train_iterations)\n",
    "            \n",
    "        total_train_iterations += 1\n",
    "    \n",
    "    print(\"Testing...\")\n",
    "    vae_model.eval()\n",
    "    test_loss = 0.0\n",
    "    since = time.time()\n",
    "    for i, (image, image_filtered, *_) in enumerate(test_loader):\n",
    "        since_iter = time.time() \n",
    "        optimiser.zero_grad()\n",
    "        \n",
    "        image = image.to(device)\n",
    "        image_filtered = image_filtered.to(device)\n",
    "\n",
    "        # VAE forward pass\n",
    "        x_hat, mu, logvar, z = vae_model(image)\n",
    "\n",
    "        # Loss\n",
    "        loss = filtered_weighted_MSE(image_filtered, x_hat, mu, logvar)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        time_iteration.append(time.time() - since_iter)\n",
    "        iter_time_mean = np.array(time_iteration).mean()\n",
    "    \n",
    "        # Log info\n",
    "        writer.add_scalar('test/loss', test_loss/((i+1)*batch_size), global_step=total_test_iterations)\n",
    "        if (i+1) % (5) == 0:\n",
    "            time_elapsed = time.time() - since\n",
    "            print(f\"Epoch: {epoch+1}/{num_epochs[-1]}, Step: {i+1}/{test_iterations}, Avg. test loss:\", \\\n",
    "                  f\"{test_loss/((i+1)*batch_size):.3f}, time: {time_elapsed:.2f}, Avg. per iter\", \\\n",
    "                  f\"{iter_time_mean:.2f}, Est. time left {iter_time_mean*(test_iterations - (i+1)):.2f}\")\n",
    "            \n",
    "        if (i+1) % (5) == 0:\n",
    "            grid = make_grid_for_tensorboard([image, image_filtered, x_hat], n_grids=1)\n",
    "            writer.add_image('test/images', grid, global_step=total_test_iterations)\n",
    "            \n",
    "        total_test_iterations += 1\n",
    "    \n",
    "    print(f\"Epoch: {epoch+1}, Training loss: {running_loss/(train_iterations*batch_size):.3f}, Test loss: {test_loss/(test_iterations*batch_size):.3f}\")\n",
    "    writer.add_scalar('loss_per_epoch/train', running_loss/(train_iterations*batch_size), global_step=epoch)\n",
    "    writer.add_scalar('loss_per_epoch/test', test_loss/(test_iterations*batch_size), global_step=epoch)\n",
    "\n",
    "    \n",
    "    # Save every 10 epochs\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        save_model_file_dir = save_model_file + f\"_{epoch+1}.pth\"\n",
    "        torch.save(vae_model.state_dict(), save_model_file_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58eeaf8",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc42698",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model_file = \"/home/patricknit/vae_models/vae_dronet_sigmoid_MSE_weighted_filtered_140.pth\"\n",
    "load_model=False\n",
    "\n",
    "if load_model:\n",
    "    vae_model.load_state_dict(torch.load(load_model_file))\n",
    "    vae_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73440ac3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vae_model.eval()\n",
    "\n",
    "# Visualise sample of images\n",
    "with torch.no_grad():\n",
    "    \n",
    "    #images, *_ = next(iter(test_loader))\n",
    "\n",
    "    x_hat, *_ = vae_model.forward_test(images)\n",
    "\n",
    "    print(images.mean(), x_hat.mean())\n",
    "    print(images.var(), x_hat.var())\n",
    "    print(images.shape)\n",
    "    for idx in range(len(images)):\n",
    "        # show images\n",
    "        imshow(images[idx])\n",
    "        imshow(x_hat[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8acf03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset = DepthImageDataset(tfrecord_folder=tfrecord_folder, batch_size=batch_size)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=1, collate_fn=collate_batch)\n",
    "\n",
    "vae_model.eval()\n",
    "# Visualise sample of images\n",
    "\n",
    "with torch.no_grad():\n",
    "        \n",
    "    images, *_ = next(iter(train_loader)) # if dataset is shuffled every iter(), essentially test \n",
    "    # VAE forward pass\n",
    "    x_hat, *_ = vae_model.forward_test(images)\n",
    "\n",
    "    print(images.mean(), x_hat.mean())\n",
    "    print(images.var(), x_hat.var())\n",
    "    print(images.shape)\n",
    "    for idx in range(len(images)):\n",
    "        # show images\n",
    "        imshow(images[idx])\n",
    "        imshow(x_hat[idx])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
