{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ea8be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-08 15:59:04.807670: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/noetic/lib:/home/patricknit/.mujoco/mjpro150/bin\n",
      "2022-03-08 15:59:04.807689: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import Adam\n",
    "\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "from torchsummary import summary\n",
    "\n",
    "from di_dataset3 import DepthImageDataset, collate_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2848f0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c88205",
   "metadata": {},
   "source": [
    "# Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82a7290",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 64\n",
    "num_epochs = 250\n",
    "batch_size = 32\n",
    "learning_rate = 1e-3\n",
    "\n",
    "save_model = True\n",
    "load_model = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f15247",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1bb310",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfrecord_folder = \"/home/patricknit/rl_data/tfrecord_wfiltered\"\n",
    "tfrecord_test_folder = \"/home/patricknit/rl_data/tfrecord_wfiltered/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82131cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DepthImageDataset(tfrecord_folder=tfrecord_folder, batch_size=batch_size) # 180 tfrecords\n",
    "test_dataset = DepthImageDataset(tfrecord_folder=tfrecord_test_folder, batch_size=batch_size) # 20 tfrecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c2e4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_train_dataset = len(train_dataset)\n",
    "len_test_dataset = len(test_dataset)\n",
    "n_training_samples = len_train_dataset * 32 # 32 samples per batch\n",
    "print(len_train_dataset, len_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526ce03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, batch_size=1, collate_fn=collate_batch)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=1, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06a2cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_loader)\n",
    "images, images_filtered, *_ = dataiter.next() # image, filtered image, height, width, depth\n",
    "images.shape, images_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5eb2b73",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def imshow(image):\n",
    "    io.imshow(image.squeeze().numpy())\n",
    "    io.show()\n",
    "\n",
    "num = 4\n",
    "for image, filtered in zip(images[:num], images_filtered[:num]):\n",
    "    imshow(image)\n",
    "    imshow(filtered)\n",
    "    \n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce9d190",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batch.squeeze().mean(), image_batch.squeeze().var()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d97d632",
   "metadata": {},
   "source": [
    "# Define Variational Autoencoder\n",
    "\n",
    "Adapted from https://github.com/microsoft/AirSim-Drone-Racing-VAE-Imitation/blob/master/racing_models/cmvae.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9e6988",
   "metadata": {},
   "source": [
    "### Dronet\n",
    "ResNet8 as encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8d396e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batch, *_ = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5587be55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Dronet(nn.Module):\n",
    "    def __init__(self, input_dim, num_outputs, include_top=True):\n",
    "        super(Dronet, self).__init__()\n",
    "        self.include_top = include_top\n",
    "        \n",
    "        print('[Dronet] Starting dronet')\n",
    "\n",
    "        self.max0 = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)  # default pool_size='2', strides=2\n",
    "\n",
    "        self.bn0 = nn.BatchNorm2d(32)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        self.bn5 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.conv0 = nn.Conv2d(input_dim, 32, kernel_size=5, stride=2, padding=2)\n",
    "        self.xavier_uniform_init(self.conv0)\n",
    "        \n",
    "        self.conv0_2 = nn.Conv2d(32, 32, kernel_size=5, stride=2, padding=2)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1) # padding='same' \n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1) # padding='same' # Todo: kernel_regularizer=tf.keras.regularizers.l2(1e-4)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=1, stride=2)\n",
    "        self.xavier_uniform_init(self.conv3)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1) # padding='same' # Todo: kernel_regularizer=tf.keras.regularizers.l2(1e-4)\n",
    "        self.conv5 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1) # padding='same' # Todo: kernel_regularizer=tf.keras.regularizers.l2(1e-4)\n",
    "        self.conv6 = nn.Conv2d(32, 64, kernel_size=1, stride=2) # padding='same'\n",
    "        self.xavier_uniform_init(self.conv6)\n",
    "\n",
    "        self.conv7 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1) # padding='same' # Todo: kernel_regularizer=tf.keras.regularizers.l2(1e-4)\n",
    "        self.conv8 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1) # padding='same' # Todo: kernel_regularizer=tf.keras.regularizers.l2(1e-4)\n",
    "        self.conv9 = nn.Conv2d(64, 128, kernel_size=1, stride=2) # padding='same'\n",
    "        self.xavier_uniform_init(self.conv9)\n",
    "\n",
    "        self.dense0 = nn.Linear(9*15*128, 64) # Todo: check size\n",
    "        self.dense1 = nn.Linear(64, 32)\n",
    "        self.dense2 = nn.Linear(32, num_outputs)\n",
    "        self.dense3 = nn.Linear(9*15*128, num_outputs)\n",
    "\n",
    "        print('[Dronet] Done with dronet')\n",
    "    \n",
    "    \n",
    "    def xavier_uniform_init(self, m):\n",
    "        \"\"\"\n",
    "        Default initialisation in Keras is glorot_uniform == xavier_uniform in Pytorch\n",
    "\n",
    "        https://discuss.pytorch.org/t/crossentropyloss-expected-object-of-type-torch-longtensor/28683/6?u=ptrblck\n",
    "        https://stackoverflow.com/questions/49433936/how-to-initialize-weights-in-pytorch\n",
    "        \"\"\"\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight, gain=nn.init.calculate_gain('linear')) # gain=nn.init.calculate_gain('relu')\n",
    "            nn.init.zeros_(m.bias)\n",
    "        return m\n",
    "\n",
    "    def forward(self, img):\n",
    "        # Input\n",
    "        \n",
    "        #print(img.shape)\n",
    "        #print(f\"- Before encoding, mean: {img.mean():.3f} var: {img.var():.3f}\")\n",
    "        x1 = self.conv0(img)\n",
    "        #print(\"x1:\", x1.shape)\n",
    "        #x1, indices = self.max0(x1)\n",
    "        x1 = self.conv0_2(x1)\n",
    "        #print(\"x1:\", x1.shape)\n",
    "\n",
    "        # First residual block\n",
    "        x2 = self.bn0(x1)\n",
    "        #print(\"x2:\", x2.shape)\n",
    "        # x2 = x1\n",
    "        x2 = torch.relu(x2)\n",
    "        #print(x2.shape)\n",
    "        x2 = self.conv1(x2)\n",
    "        #print(x2.shape)\n",
    "\n",
    "        x2 = self.bn1(x2)\n",
    "        #print(x2.shape)\n",
    "        x2 = torch.relu(x2)\n",
    "        #print(x2.shape)\n",
    "        x2 = self.conv2(x2)\n",
    "        #print(x2.shape)\n",
    "        \n",
    "        \n",
    "        x1 = self.conv3(x1)\n",
    "        #print(f\"x1: {x1.shape}, x2: {x2.shape}\") # 32, 34, 60\n",
    "        x3 = x1+x2\n",
    "        #print(\"x3:\", x3.shape)\n",
    "\n",
    "        # Second residual block\n",
    "        x4 = self.bn2(x3)\n",
    "        #print(\"x4:\", x4.shape)\n",
    "        # x4 = x3\n",
    "        x4 = torch.relu(x4)\n",
    "        x4 = self.conv4(x4)\n",
    "        #print(x4.shape)\n",
    "\n",
    "        x4 = self.bn3(x4)\n",
    "        #print(x4.shape)\n",
    "        x4 = torch.relu(x4)\n",
    "        x4 = self.conv5(x4)\n",
    "        #print(x4.shape)\n",
    "        \n",
    "        x3 = self.conv6(x3)\n",
    "        #print(f\"x3: {x3.shape}, x4: {x4.shape}\")\n",
    "        x5 = x3+x4 # 34, 60, 64\n",
    "\n",
    "        # Third residual block\n",
    "        x6 = self.bn4(x5)\n",
    "        # x6 = x5\n",
    "        x6 = torch.relu(x6)\n",
    "        x6 = self.conv7(x6)\n",
    "\n",
    "        x6 = self.bn5(x6)\n",
    "        x6 = torch.relu(x6)\n",
    "        x6 = self.conv8(x6)\n",
    "\n",
    "        x5 = self.conv9(x5)\n",
    "        #print(f\"x5: {x5.shape}, x6: {x6.shape}\")\n",
    "        x7 = x5+x6 # 17, 30, 128\n",
    "\n",
    "        x = x7.view(x7.size(0), -1)\n",
    "        #print(f\"x before linear: {x.shape}\")\n",
    "        #print(f\"- After encoding, mean: {x.mean():.3f} var: {x.var():.3f}\")\n",
    "\n",
    "        if self.include_top:\n",
    "            x = torch.relu(x)\n",
    "            # x = tf.keras.layers.Dropout(0.5)(x)\n",
    "            x = self.dense0(x)\n",
    "            x = self.dense1(x)\n",
    "            #gate_pose = self.dense2(x)\n",
    "            x_enc = self.dense2(x)\n",
    "            # phi_rel = self.dense_phi_rel(x)\n",
    "            # gate_pose = tf.concat([gate_pose, phi_rel], 1)\n",
    "            #print(f\"x_enc: {x.shape}\")\n",
    "            return x_enc\n",
    "        else:\n",
    "            x = self.dense3(x)\n",
    "            #print(f\"x_enc: {x.shape}\")\n",
    "            return x\n",
    "\n",
    "dronet = Dronet(input_dim=1, num_outputs=latent_dim*2, include_top=True)\n",
    "summary(dronet, input_size=(1, 270, 480), batch_size=-1, device=device.type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224f0812",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642ed80c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class ImgDecoder(nn.Module):\n",
    "    def __init__(self, input_dim=100, batch=32):\n",
    "        super(ImgDecoder, self).__init__()\n",
    "        print('[ImgDecoder] Starting create_model')\n",
    "        self.dense = nn.Linear(input_dim, 9*15*128)\n",
    "        \n",
    "        self.bn0 = nn.BatchNorm2d(128)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        self.bn5 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.unpool = nn.MaxUnpool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.deconv1 = nn.ConvTranspose2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2, padding=(2,2), output_padding=(0,1), dilation=1)\n",
    "        self.deconv3 = nn.ConvTranspose2d(64, 64, kernel_size=6, stride=2, padding=(2,2), output_padding=(0,0), dilation=1)\n",
    "        self.deconv4 = nn.ConvTranspose2d(64, 32, kernel_size=6, stride=2, padding=(2,2), output_padding=(0,0), dilation=1)\n",
    "        self.deconv5 = nn.ConvTranspose2d(32, 32, kernel_size=5, stride=2, padding=(2,2), output_padding=(0,1), dilation=1)\n",
    "        self.deconv6 = nn.ConvTranspose2d(32, 16, kernel_size=6, stride=2, padding=(2,2), output_padding=(0,0))\n",
    "        self.deconv7 = nn.ConvTranspose2d(16, 1, kernel_size=5, stride=1, padding=2) # tanh activation or sigmoid\n",
    "\n",
    "        print('[ImgDecoder] Done with create_model')\n",
    "\n",
    "    def forward(self, z):\n",
    "        #print(\"z: \", z.shape)\n",
    "        x = self.dense(z)\n",
    "        #print(x.shape)\n",
    "        x = x.view(x.size(0), 128, 9, 15)\n",
    "        #print(f\"- Before deconv, mean: {x.mean():.3f} var: {x.var():.3f}\")\n",
    "        \n",
    "        x = self.deconv1(x)\n",
    "        x = torch.relu(x)\n",
    "        #print(\"after deconv1\", x.shape)\n",
    "        \n",
    "        x = self.deconv2(x)\n",
    "        x = torch.relu(x)\n",
    "        #print(\"after deconv2\", x.shape)\n",
    "        \n",
    "        x = self.deconv3(x)\n",
    "        x = torch.relu(x)\n",
    "#         print(\"after deconv3\", x.shape)\n",
    "\n",
    "        x = self.deconv4(x)\n",
    "        x = torch.relu(x)\n",
    "#         print(\"after deconv4\", x.shape)\n",
    "\n",
    "        x = self.deconv5(x)\n",
    "        #print(\"indices\", indices.shape)\n",
    "        #x = self.unpool(x, indices, output_size=(x.size(0), 1, 135, 240))\n",
    "#       print(\"after deconv5\", x.shape)\n",
    "        x = torch.relu(x)\n",
    "        #print(\"after unpool\", x.shape)\n",
    "        \n",
    "        x = self.deconv6(x)\n",
    "        x = torch.relu(x)\n",
    "#         print(\"after deconv6\", x.shape)\n",
    "\n",
    "        x = self.deconv7(x)\n",
    "        #print(f\"- After deconv 7, mean: {x.mean():.3f} var: {x.var():.3f}\")\n",
    "        x = torch.sigmoid(x)\n",
    "        #print(f\"- After sigmoid, mean: {x.mean():.3f} var: {x.var():.3f}\")\n",
    "        #print(\"after deconv7\", x.shape)\n",
    "        return x\n",
    "\n",
    "img_decoder = ImgDecoder(input_dim=latent_dim, batch=1)\n",
    "summary(img_decoder, (1,latent_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc98b84",
   "metadata": {},
   "source": [
    "## VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997b68c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super(Lambda, self).__init__()\n",
    "        self.func = func\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.func(x)\n",
    "        \n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, n_z):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # create the 3 base models:\n",
    "        self.q_img = Dronet(input_dim=1, num_outputs=n_z*2, include_top=True)\n",
    "        self.p_img = ImgDecoder(input_dim=n_z)\n",
    "        \n",
    "        # Create sampler\n",
    "        self.mean_params = Lambda(lambda x: x[:, :n_z])\n",
    "        self.logvar_params = Lambda(lambda x: x[:, n_z:])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Possible modes for reconstruction:\n",
    "        # img -> img\n",
    "\n",
    "        x = self.q_img(x)\n",
    "        \n",
    "        means = self.mean_params(x)\n",
    "        logvar = self.logvar_params(x)\n",
    "        stddev = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(stddev)\n",
    "        z = means + eps * stddev\n",
    "        #print(z.shape)\n",
    "        #print(\"OK\")\n",
    "        img_recon = self.p_img(z)\n",
    "        \n",
    "        return img_recon, means, logvar, z\n",
    "    \n",
    "    def forward_test(self, x):\n",
    "        x = self.q_img(x)\n",
    "        \n",
    "        means = self.mean_params(x)\n",
    "        logvar = self.logvar_params(x)\n",
    "        stddev = torch.exp(0.5 * logvar)\n",
    "        eps = torch.zeros_like(stddev)\n",
    "        z = means + eps * stddev\n",
    "        #print(z.shape)\n",
    "        #print(\"OK\")\n",
    "        img_recon = self.p_img(z)\n",
    "        return img_recon, means, logvar, z\n",
    "        \n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.q_img(x)\n",
    "        means = self.mean_params(x)\n",
    "        stddev = torch.exp(0.5 * self.stddev_params(x))\n",
    "        eps = torch.randn_like(stddev)\n",
    "        z = means + eps * stddev\n",
    "        return z, means, stddev\n",
    "\n",
    "    def decode(self, z, mode):\n",
    "        # Possible modes for reconstruction:\n",
    "        # z -> img\n",
    "        img_recon = self.p_img(z)\n",
    "        return img_recon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadac3a2",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9d8b94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vae_model = VAE(n_z=latent_dim)\n",
    "\n",
    "load_model_file = \"/home/patricknit/vae_models/vae_dronet_sigmoid_MSE_weighted_90.pth\"\n",
    "load_model = True\n",
    "\n",
    "if load_model:\n",
    "    vae_model.load_state_dict(torch.load(load_model_file))\n",
    "else:\n",
    "    vae_model.train()\n",
    "\n",
    "optimiser = torch.optim.Adam(vae_model.parameters(), lr=learning_rate)\n",
    "summary(vae_model, (1, 270, 480))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f69f90e",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5515c863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def depth_gain(x):\n",
    "    return 1/(x + 0.5) - 0.5\n",
    "\n",
    "def BCELoss(x, x_hat, mean, log_var):\n",
    "    reproduction_loss = nn.BCELoss(reduction='sum')(x_hat, x)\n",
    "    KLD      = 0.5 * torch.sum(1+ log_var - mean.pow(2) - log_var.exp())\n",
    "\n",
    "    return reproduction_loss - KLD\n",
    "\n",
    "def MSE(x, x_hat, mu, logvar):\n",
    "    #rep = (x_hat - x)**2\n",
    "    reproduction_loss = F.mse_loss(x_hat, x, reduction=\"sum\")\n",
    "    kl_divergence = 0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return reproduction_loss - kl_divergence\n",
    "\n",
    "def weighted_MSE(x, x_hat, mu, logvar):\n",
    "    reproduction_loss_ref = F.mse_loss(x_hat, x, reduction=\"sum\")\n",
    "    \n",
    "    reproduction_loss = (depth_gain(x) * torch.square(x_hat - x)).sum()\n",
    "    kl_divergence = 0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "    #print(reproduction_loss, reproduction_loss_ref)\n",
    "    return reproduction_loss - kl_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4552d13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "time_iteration = []\n",
    "# train_dataset = DepthImageDataset(tfrecord_folder=tfrecord_folder, batch_size=batch_size)\n",
    "# train_loader = DataLoader(dataset=train_dataset, batch_size=1, collate_fn=collate_batch)\n",
    "# train_loader = next(iter(train_loader))\n",
    "\n",
    "save_model_file = \"/home/patricknit/vae_models/vae_dronet_sigmoid_MSE_weighted\"\n",
    "\n",
    "if load_model:\n",
    "    start = load_model_file.rfind(\"_\") + 1\n",
    "    end = load_model_file.rfind(\".\")\n",
    "    epoch_start = int(\"\".join([char for char in load_model_file[start:end]]))\n",
    "    num_epochs = [epoch_start, num_epochs]\n",
    "else:\n",
    "    num_epochs = [num_epochs]\n",
    "    \n",
    "for epoch in range(*num_epochs):\n",
    "    overall_loss = 0\n",
    "    overall_elbo = 0\n",
    "    since = time.time()\n",
    "    \n",
    "    # Random file each time\n",
    "    train_dataset = DepthImageDataset(tfrecord_folder=tfrecord_folder, batch_size=batch_size)\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=1, collate_fn=collate_batch)\n",
    "    n_iterations = len(train_loader)\n",
    "    \n",
    "    for i, (image, *_) in enumerate(train_loader):\n",
    "        since_iter = time.time()\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        # VAE forward pass\n",
    "        x_hat, mu, logvar, z = vae_model(image)\n",
    "\n",
    "        # Loss\n",
    "        loss = weighted_MSE(image, x_hat, mu, logvar)\n",
    "        #print(loss)\n",
    "\n",
    "        overall_loss += loss\n",
    "\n",
    "        # Update weights\n",
    "        \n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        \n",
    "        \n",
    "        time_iteration.append(time.time() - since_iter)\n",
    "        iter_time_mean = np.array(time_iteration).mean()\n",
    "        \n",
    "        if (i+1) % (5) == 0:\n",
    "            time_elapsed = time.time() - since\n",
    "            print(f\"Epoch: {epoch+1}/{num_epochs}, Step: {i+1}/{n_iterations}, Avg loss: {overall_loss/((i+1)*batch_size):.3f}, time: {time_elapsed:.2f}, Avg. per iter {iter_time_mean:.2f}, Est. time left {iter_time_mean*(n_iterations - (i+1)):.2f}\")\n",
    "            print()\n",
    "            \n",
    "        if (i+1) % (15) == 0:\n",
    "            imshow(image[0])\n",
    "            imshow(x_hat[0].detach())\n",
    "            \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        save_model_file_dir = save_model_file + f\"_{epoch+1}.pth\"\n",
    "        torch.save(vae_model.state_dict(), save_model_file_dir)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1e4c74",
   "metadata": {},
   "source": [
    "# Some results\n",
    "\n",
    "### BCE\n",
    "- 10 epochs, no batch norm: 60191.195\n",
    "\n",
    "### BCE with unpooling\n",
    "- 10 epochs, clear reconstruction but image indices carried through VAE. Faster training than BCE\n",
    "\n",
    "### ReLU + MSE loss\n",
    "- Avg loss: 2629.289, very poor reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d4a372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(vae_model.state_dict(), save_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc42698",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load_model_file = \"/home/patricknit/vae_models/vae_dronet_sigmoid_MSE_weighted_57.pth\"\n",
    "load_model=False\n",
    "\n",
    "if load_model:\n",
    "    vae_model.load_state_dict(torch.load(load_model_file))\n",
    "    vae_model.eval()\n",
    "else:\n",
    "    vae_model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58eeaf8",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73440ac3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vae_model.eval()\n",
    "\n",
    "# Visualise sample of images\n",
    "with torch.no_grad():\n",
    "    \n",
    "    images, *_ = next(iter(test_loader))\n",
    "\n",
    "    x_hat, *_ = vae_model.forward_test(images)\n",
    "\n",
    "    print(images.mean(), x_hat.mean())\n",
    "    print(images.var(), x_hat.var())\n",
    "    print(images.shape)\n",
    "    for idx in range(len(images)):\n",
    "        # show images\n",
    "        imshow(images[idx])\n",
    "        imshow(x_hat[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8acf03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset = DepthImageDataset(tfrecord_folder=tfrecord_folder, batch_size=batch_size)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=1, collate_fn=collate_batch)\n",
    "\n",
    "vae_model.eval()\n",
    "# Visualise sample of images\n",
    "\n",
    "with torch.no_grad():\n",
    "        \n",
    "    images, *_ = next(iter(train_loader)) # if dataset is shuffled every iter(), essentially test \n",
    "    # VAE forward pass\n",
    "    x_hat, *_ = vae_model.forward_test(images)\n",
    "\n",
    "    print(images.mean(), x_hat.mean())\n",
    "    print(images.var(), x_hat.var())\n",
    "    print(images.shape)\n",
    "    for idx in range(len(images)):\n",
    "        # show images\n",
    "        imshow(images[idx])\n",
    "        imshow(x_hat[idx])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
