{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53ea8be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import Adam\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "from torchsummary import summary\n",
    "\n",
    "from di_dataset3 import DepthImageDataset, collate_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2848f0f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c88205",
   "metadata": {},
   "source": [
    "# Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a82a7290",
   "metadata": {},
   "outputs": [],
   "source": [
    "LINUX = False\n",
    "\n",
    "latent_dim = 64\n",
    "num_epochs = 250\n",
    "batch_size = 32\n",
    "learning_rate = 1e-3\n",
    "\n",
    "save_model = True\n",
    "load_model = False\n",
    "\n",
    "if LINUX:\n",
    "    tfrecord_folder = \"/home/patricknit/rl_data/tfrecord_wfiltered\"\n",
    "    tfrecord_test_folder = \"/home/patricknit/rl_data/tfrecord_wfiltered/test\"\n",
    "\n",
    "    save_model_file = \"/home/patricknit/vae_models/vae_dronet_sigmoid_MSE_weighted\"\n",
    "    load_model_file = \"/home/patricknit/vae_models/vae_dronet_sigmoid_MSE_weighted_90.pth\"\n",
    "\n",
    "else:\n",
    "    base_path = \"/Users/patricknitschke/Library/CloudStorage/OneDrive-NTNU/NTNU/Kybernetikk og robotikk/Master/Thesis/Code\"\n",
    "    tfrecord_folder = base_path + \"/rl_data/tfrecord_wfiltered\"\n",
    "    tfrecord_test_folder = tfrecord_folder\n",
    "    \n",
    "    save_model_file = base_path + \"/vae_models/vae_dronet_sigmoid_MSE_weighted_filtered\"\n",
    "    load_model_file = base_path + \"/vae_models/vae_dronet_sigmoid_MSE_weighted_90.pth\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f15247",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d82131cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tfrecords... \t/Users/patricknitschke/Library/CloudStorage/OneDrive-NTNU/NTNU/Kybernetikk og robotikk/Master/Thesis/Code/Pytorch/rl_data/tfrecord_wfiltered/data4.tfrecords\n",
      "Metal device set to: Apple M1\n",
      "Iterating length... \t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-08 22:30:04.018292: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-03-08 22:30:04.018859: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2022-03-08 22:30:04.183091: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "/Users/patricknitschke/Library/CloudStorage/OneDrive-NTNU/NTNU/Kybernetikk og robotikk/Master/Thesis/Code/Pytorch/rl_data/tfrecord_wfiltered/data4.tfrecords; No such file or directory [Op:IteratorGetNext]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m DepthImageDataset(tfrecord_folder\u001b[38;5;241m=\u001b[39mtfrecord_folder, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, one_tfrecord\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m# 180 tfrecords\u001b[39;00m\n\u001b[1;32m      2\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m DepthImageDataset(tfrecord_folder\u001b[38;5;241m=\u001b[39mtfrecord_test_folder, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, one_tfrecord\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-NTNU/NTNU/Kybernetikk og robotikk/Master/Thesis/Code/Pytorch/VAE/di_dataset3.py:22\u001b[0m, in \u001b[0;36mDepthImageDataset.__init__\u001b[0;34m(self, tfrecord_folder, batch_size, shuffle, one_tfrecord)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28msuper\u001b[39m(DepthImageDataset)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtfrecord_folder \u001b[38;5;241m=\u001b[39m tfrecord_folder\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_tfrecords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_shuffle_and_repeat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_tfrecord\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mone_tfrecord\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-NTNU/NTNU/Kybernetikk og robotikk/Master/Thesis/Code/Pytorch/VAE/di_dataset3.py:63\u001b[0m, in \u001b[0;36mDepthImageDataset.load_tfrecords\u001b[0;34m(self, is_shuffle_and_repeat, shuffle_buffer_size, prefetch_buffer_size_multiplier, batch_size, one_tfrecord)\u001b[0m\n\u001b[1;32m     60\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mprefetch(buffer_size\u001b[38;5;241m=\u001b[39mprefetch_buffer_size_multiplier \u001b[38;5;241m*\u001b[39m batch_size)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIterating length... \u001b[39m\u001b[38;5;124m'\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 63\u001b[0m data_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDone:\u001b[39m\u001b[38;5;124m'\u001b[39m, data_len)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dataset, data_len\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-NTNU/NTNU/Kybernetikk og robotikk/Master/Thesis/Code/Pytorch/VAE/di_dataset3.py:63\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     60\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mprefetch(buffer_size\u001b[38;5;241m=\u001b[39mprefetch_buffer_size_multiplier \u001b[38;5;241m*\u001b[39m batch_size)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIterating length... \u001b[39m\u001b[38;5;124m'\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 63\u001b[0m data_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m dataset)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDone:\u001b[39m\u001b[38;5;124m'\u001b[39m, data_len)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dataset, data_len\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch-tf/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py:836\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    835\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 836\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    837\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOutOfRangeError:\n\u001b[1;32m    838\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch-tf/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py:819\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;66;03m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;66;03m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecution_mode(context\u001b[38;5;241m.\u001b[39mSYNC):\n\u001b[0;32m--> 819\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator_get_next\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    820\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    821\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    824\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    825\u001b[0m     \u001b[38;5;66;03m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[1;32m    826\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_element_spec\u001b[38;5;241m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch-tf/lib/python3.8/site-packages/tensorflow/python/ops/gen_dataset_ops.py:2923\u001b[0m, in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2921\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   2922\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 2923\u001b[0m   \u001b[43m_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from_not_ok_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2924\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_FallbackException:\n\u001b[1;32m   2925\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch-tf/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:7186\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   7185\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 7186\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: /Users/patricknitschke/Library/CloudStorage/OneDrive-NTNU/NTNU/Kybernetikk og robotikk/Master/Thesis/Code/Pytorch/rl_data/tfrecord_wfiltered/data4.tfrecords; No such file or directory [Op:IteratorGetNext]"
     ]
    }
   ],
   "source": [
    "train_dataset = DepthImageDataset(tfrecord_folder=tfrecord_folder, batch_size=batch_size, one_tfrecord=True) # 180 tfrecords\n",
    "test_dataset = DepthImageDataset(tfrecord_folder=tfrecord_test_folder, batch_size=batch_size, one_tfrecord=True) # 20 tfrecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c2e4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_train_dataset = len(train_dataset)\n",
    "len_test_dataset = len(test_dataset)\n",
    "n_training_samples = len_train_dataset * 32 # 32 samples per batch\n",
    "print(len_train_dataset, len_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526ce03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, batch_size=1, collate_fn=collate_batch)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=1, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06a2cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_loader)\n",
    "images, images_filtered, *_ = dataiter.next() # image, filtered image, height, width, depth\n",
    "images.shape, images_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5eb2b73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def imshow(image):\n",
    "    io.imshow(image.squeeze().numpy())\n",
    "    io.show()\n",
    "\n",
    "num = 4\n",
    "for image, filtered in zip(images[:num], images_filtered[:num]):\n",
    "    imshow(image)\n",
    "    imshow(filtered)\n",
    "    \n",
    "image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d97d632",
   "metadata": {},
   "source": [
    "# Define Variational Autoencoder\n",
    "\n",
    "Adapted from https://github.com/microsoft/AirSim-Drone-Racing-VAE-Imitation/blob/master/racing_models/cmvae.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9e6988",
   "metadata": {},
   "source": [
    "### Dronet\n",
    "ResNet8 as encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5587be55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Dronet(nn.Module):\n",
    "    def __init__(self, input_dim, num_outputs, include_top=True):\n",
    "        super(Dronet, self).__init__()\n",
    "        self.include_top = include_top\n",
    "        \n",
    "        print('[Dronet] Starting dronet')\n",
    "\n",
    "        self.max0 = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)  # default pool_size='2', strides=2\n",
    "\n",
    "        self.bn0 = nn.BatchNorm2d(32)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        self.bn5 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.conv0 = nn.Conv2d(input_dim, 32, kernel_size=5, stride=2, padding=2)\n",
    "        self.xavier_uniform_init(self.conv0)\n",
    "        \n",
    "        self.conv0_2 = nn.Conv2d(32, 32, kernel_size=5, stride=2, padding=2)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1) # padding='same' \n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1) # padding='same' # Todo: kernel_regularizer=tf.keras.regularizers.l2(1e-4)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=1, stride=2)\n",
    "        self.xavier_uniform_init(self.conv3)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1) # padding='same' # Todo: kernel_regularizer=tf.keras.regularizers.l2(1e-4)\n",
    "        self.conv5 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1) # padding='same' # Todo: kernel_regularizer=tf.keras.regularizers.l2(1e-4)\n",
    "        self.conv6 = nn.Conv2d(32, 64, kernel_size=1, stride=2) # padding='same'\n",
    "        self.xavier_uniform_init(self.conv6)\n",
    "\n",
    "        self.conv7 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1) # padding='same' # Todo: kernel_regularizer=tf.keras.regularizers.l2(1e-4)\n",
    "        self.conv8 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1) # padding='same' # Todo: kernel_regularizer=tf.keras.regularizers.l2(1e-4)\n",
    "        self.conv9 = nn.Conv2d(64, 128, kernel_size=1, stride=2) # padding='same'\n",
    "        self.xavier_uniform_init(self.conv9)\n",
    "\n",
    "        self.dense0 = nn.Linear(9*15*128, 64) # Todo: check size\n",
    "        self.dense1 = nn.Linear(64, 32)\n",
    "        self.dense2 = nn.Linear(32, num_outputs)\n",
    "        self.dense3 = nn.Linear(9*15*128, num_outputs)\n",
    "\n",
    "        print('[Dronet] Done with dronet')\n",
    "    \n",
    "    \n",
    "    def xavier_uniform_init(self, m):\n",
    "        \"\"\"\n",
    "        Default initialisation in Keras is glorot_uniform == xavier_uniform in Pytorch\n",
    "\n",
    "        https://discuss.pytorch.org/t/crossentropyloss-expected-object-of-type-torch-longtensor/28683/6?u=ptrblck\n",
    "        https://stackoverflow.com/questions/49433936/how-to-initialize-weights-in-pytorch\n",
    "        \"\"\"\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight, gain=nn.init.calculate_gain('linear')) # gain=nn.init.calculate_gain('relu')\n",
    "            nn.init.zeros_(m.bias)\n",
    "        return m\n",
    "\n",
    "    def forward(self, img):\n",
    "        # Input\n",
    "        \n",
    "        #print(img.shape)\n",
    "        #print(f\"- Before encoding, mean: {img.mean():.3f} var: {img.var():.3f}\")\n",
    "        x1 = self.conv0(img)\n",
    "        #print(\"x1:\", x1.shape)\n",
    "        #x1, indices = self.max0(x1)\n",
    "        x1 = self.conv0_2(x1)\n",
    "        #print(\"x1:\", x1.shape)\n",
    "\n",
    "        # First residual block\n",
    "        x2 = self.bn0(x1)\n",
    "        #print(\"x2:\", x2.shape)\n",
    "        # x2 = x1\n",
    "        x2 = torch.relu(x2)\n",
    "        #print(x2.shape)\n",
    "        x2 = self.conv1(x2)\n",
    "        #print(x2.shape)\n",
    "\n",
    "        x2 = self.bn1(x2)\n",
    "        #print(x2.shape)\n",
    "        x2 = torch.relu(x2)\n",
    "        #print(x2.shape)\n",
    "        x2 = self.conv2(x2)\n",
    "        #print(x2.shape)\n",
    "        \n",
    "        \n",
    "        x1 = self.conv3(x1)\n",
    "        #print(f\"x1: {x1.shape}, x2: {x2.shape}\") # 32, 34, 60\n",
    "        x3 = x1+x2\n",
    "        #print(\"x3:\", x3.shape)\n",
    "\n",
    "        # Second residual block\n",
    "        x4 = self.bn2(x3)\n",
    "        #print(\"x4:\", x4.shape)\n",
    "        # x4 = x3\n",
    "        x4 = torch.relu(x4)\n",
    "        x4 = self.conv4(x4)\n",
    "        #print(x4.shape)\n",
    "\n",
    "        x4 = self.bn3(x4)\n",
    "        #print(x4.shape)\n",
    "        x4 = torch.relu(x4)\n",
    "        x4 = self.conv5(x4)\n",
    "        #print(x4.shape)\n",
    "        \n",
    "        x3 = self.conv6(x3)\n",
    "        #print(f\"x3: {x3.shape}, x4: {x4.shape}\")\n",
    "        x5 = x3+x4 # 34, 60, 64\n",
    "\n",
    "        # Third residual block\n",
    "        x6 = self.bn4(x5)\n",
    "        # x6 = x5\n",
    "        x6 = torch.relu(x6)\n",
    "        x6 = self.conv7(x6)\n",
    "\n",
    "        x6 = self.bn5(x6)\n",
    "        x6 = torch.relu(x6)\n",
    "        x6 = self.conv8(x6)\n",
    "\n",
    "        x5 = self.conv9(x5)\n",
    "        #print(f\"x5: {x5.shape}, x6: {x6.shape}\")\n",
    "        x7 = x5+x6 # 17, 30, 128\n",
    "\n",
    "        x = x7.view(x7.size(0), -1)\n",
    "        #print(f\"x before linear: {x.shape}\")\n",
    "        #print(f\"- After encoding, mean: {x.mean():.3f} var: {x.var():.3f}\")\n",
    "\n",
    "        if self.include_top:\n",
    "            x = torch.relu(x)\n",
    "            # x = tf.keras.layers.Dropout(0.5)(x)\n",
    "            x = self.dense0(x)\n",
    "            x = self.dense1(x)\n",
    "            #gate_pose = self.dense2(x)\n",
    "            x_enc = self.dense2(x)\n",
    "            # phi_rel = self.dense_phi_rel(x)\n",
    "            # gate_pose = tf.concat([gate_pose, phi_rel], 1)\n",
    "            #print(f\"x_enc: {x.shape}\")\n",
    "            return x_enc\n",
    "        else:\n",
    "            x = self.dense3(x)\n",
    "            #print(f\"x_enc: {x.shape}\")\n",
    "            return x\n",
    "\n",
    "dronet = Dronet(input_dim=1, num_outputs=latent_dim*2, include_top=True)\n",
    "summary(dronet, input_size=(1, 270, 480), batch_size=-1, device=device.type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224f0812",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642ed80c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class ImgDecoder(nn.Module):\n",
    "    def __init__(self, input_dim=100, batch=32):\n",
    "        super(ImgDecoder, self).__init__()\n",
    "        print('[ImgDecoder] Starting create_model')\n",
    "        self.dense = nn.Linear(input_dim, 9*15*128)\n",
    "        \n",
    "        self.bn0 = nn.BatchNorm2d(128)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        self.bn5 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.unpool = nn.MaxUnpool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.deconv1 = nn.ConvTranspose2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(128, 64, kernel_size=5, stride=2, padding=(2,2), output_padding=(0,1), dilation=1)\n",
    "        self.deconv3 = nn.ConvTranspose2d(64, 64, kernel_size=6, stride=2, padding=(2,2), output_padding=(0,0), dilation=1)\n",
    "        self.deconv4 = nn.ConvTranspose2d(64, 32, kernel_size=6, stride=2, padding=(2,2), output_padding=(0,0), dilation=1)\n",
    "        self.deconv5 = nn.ConvTranspose2d(32, 32, kernel_size=5, stride=2, padding=(2,2), output_padding=(0,1), dilation=1)\n",
    "        self.deconv6 = nn.ConvTranspose2d(32, 16, kernel_size=6, stride=2, padding=(2,2), output_padding=(0,0))\n",
    "        self.deconv7 = nn.ConvTranspose2d(16, 1, kernel_size=5, stride=1, padding=2) # tanh activation or sigmoid\n",
    "\n",
    "        print('[ImgDecoder] Done with create_model')\n",
    "\n",
    "    def forward(self, z):\n",
    "        #print(\"z: \", z.shape)\n",
    "        x = self.dense(z)\n",
    "        #print(x.shape)\n",
    "        x = x.view(x.size(0), 128, 9, 15)\n",
    "        #print(f\"- Before deconv, mean: {x.mean():.3f} var: {x.var():.3f}\")\n",
    "        \n",
    "        x = self.deconv1(x)\n",
    "        x = torch.relu(x)\n",
    "        #print(\"after deconv1\", x.shape)\n",
    "        \n",
    "        x = self.deconv2(x)\n",
    "        x = torch.relu(x)\n",
    "        #print(\"after deconv2\", x.shape)\n",
    "        \n",
    "        x = self.deconv3(x)\n",
    "        x = torch.relu(x)\n",
    "#         print(\"after deconv3\", x.shape)\n",
    "\n",
    "        x = self.deconv4(x)\n",
    "        x = torch.relu(x)\n",
    "#         print(\"after deconv4\", x.shape)\n",
    "\n",
    "        x = self.deconv5(x)\n",
    "        #print(\"indices\", indices.shape)\n",
    "        #x = self.unpool(x, indices, output_size=(x.size(0), 1, 135, 240))\n",
    "#       print(\"after deconv5\", x.shape)\n",
    "        x = torch.relu(x)\n",
    "        #print(\"after unpool\", x.shape)\n",
    "        \n",
    "        x = self.deconv6(x)\n",
    "        x = torch.relu(x)\n",
    "#         print(\"after deconv6\", x.shape)\n",
    "\n",
    "        x = self.deconv7(x)\n",
    "        #print(f\"- After deconv 7, mean: {x.mean():.3f} var: {x.var():.3f}\")\n",
    "        x = torch.sigmoid(x)\n",
    "        #print(f\"- After sigmoid, mean: {x.mean():.3f} var: {x.var():.3f}\")\n",
    "        #print(\"after deconv7\", x.shape)\n",
    "        return x\n",
    "\n",
    "img_decoder = ImgDecoder(input_dim=latent_dim, batch=1)\n",
    "summary(img_decoder, (1,latent_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc98b84",
   "metadata": {},
   "source": [
    "## VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997b68c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super(Lambda, self).__init__()\n",
    "        self.func = func\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.func(x)\n",
    "        \n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, n_z):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # create the 3 base models:\n",
    "        self.q_img = Dronet(input_dim=1, num_outputs=n_z*2, include_top=True)\n",
    "        self.p_img = ImgDecoder(input_dim=n_z)\n",
    "        \n",
    "        # Create sampler\n",
    "        self.mean_params = Lambda(lambda x: x[:, :n_z])\n",
    "        self.logvar_params = Lambda(lambda x: x[:, n_z:])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Possible modes for reconstruction:\n",
    "        # img -> img\n",
    "\n",
    "        x = self.q_img(x)\n",
    "        \n",
    "        means = self.mean_params(x)\n",
    "        logvar = self.logvar_params(x)\n",
    "        stddev = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(stddev)\n",
    "        z = means + eps * stddev\n",
    "        #print(z.shape)\n",
    "        #print(\"OK\")\n",
    "        img_recon = self.p_img(z)\n",
    "        \n",
    "        return img_recon, means, logvar, z\n",
    "    \n",
    "    def forward_test(self, x):\n",
    "        x = self.q_img(x)\n",
    "        \n",
    "        means = self.mean_params(x)\n",
    "        logvar = self.logvar_params(x)\n",
    "        stddev = torch.exp(0.5 * logvar)\n",
    "        eps = torch.zeros_like(stddev)\n",
    "        z = means + eps * stddev\n",
    "        #print(z.shape)\n",
    "        #print(\"OK\")\n",
    "        img_recon = self.p_img(z)\n",
    "        return img_recon, means, logvar, z\n",
    "        \n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.q_img(x)\n",
    "        means = self.mean_params(x)\n",
    "        stddev = torch.exp(0.5 * self.stddev_params(x))\n",
    "        eps = torch.randn_like(stddev)\n",
    "        z = means + eps * stddev\n",
    "        return z, means, stddev\n",
    "\n",
    "    def decode(self, z, mode):\n",
    "        # Possible modes for reconstruction:\n",
    "        # z -> img\n",
    "        img_recon = self.p_img(z)\n",
    "        return img_recon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadac3a2",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9d8b94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vae_model = VAE(n_z=latent_dim)\n",
    "\n",
    "if load_model:\n",
    "    vae_model.load_state_dict(torch.load(load_model_file))\n",
    "    print(f\"Loaded model: {load_model_file}\")\n",
    "\n",
    "optimiser = torch.optim.Adam(vae_model.parameters(), lr=learning_rate)\n",
    "summary(vae_model, (1, 270, 480))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f69f90e",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5515c863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def depth_gain(x):\n",
    "    return 1/(x + 0.5) - 0.5\n",
    "\n",
    "def BCELoss(x, x_hat, mean, log_var):\n",
    "    reproduction_loss = nn.BCELoss(reduction='sum')(x_hat, x)\n",
    "    KLD      = 0.5 * torch.sum(1+ log_var - mean.pow(2) - log_var.exp())\n",
    "\n",
    "    return reproduction_loss - KLD\n",
    "\n",
    "def MSE(x, x_hat, mu, logvar):\n",
    "    #rep = (x_hat - x)**2\n",
    "    reproduction_loss = F.mse_loss(x_hat, x, reduction=\"sum\")\n",
    "    kl_divergence = 0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return reproduction_loss - kl_divergence\n",
    "\n",
    "def weighted_MSE(x, x_hat, mu, logvar):\n",
    "    reproduction_loss_ref = F.mse_loss(x_hat, x, reduction=\"sum\")\n",
    "    \n",
    "    reproduction_loss = (depth_gain(x) * torch.square(x_hat - x)).sum()\n",
    "    kl_divergence = 0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "    #print(reproduction_loss, reproduction_loss_ref)\n",
    "    return reproduction_loss - kl_divergence\n",
    "\n",
    "def filtered_weighted_MSE(filtered_x, x_hat, mu, logvar): \n",
    "    reproduction_loss = (depth_gain(filtered_x) * torch.square(x_hat - filtered_x)).sum()\n",
    "    kl_divergence = 0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "    #print(reproduction_loss, reproduction_loss_ref)\n",
    "    return reproduction_loss - kl_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4552d13",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "time_iteration = []\n",
    "# train_dataset = DepthImageDataset(tfrecord_folder=tfrecord_folder, batch_size=batch_size)\n",
    "# train_loader = DataLoader(dataset=train_dataset, batch_size=1, collate_fn=collate_batch)\n",
    "# train_loader = next(iter(train_loader))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    overall_loss = 0\n",
    "    overall_elbo = 0\n",
    "    since = time.time()\n",
    "    \n",
    "    # Random file each time\n",
    "    train_dataset = DepthImageDataset(tfrecord_folder=tfrecord_folder, batch_size=batch_size, one_tfrecord=True)\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=1, collate_fn=collate_batch)\n",
    "    n_iterations = len(train_loader)\n",
    "    \n",
    "    for i, (image, image_filtered, *_) in enumerate(train_loader):\n",
    "        since_iter = time.time()\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        # VAE forward pass\n",
    "        x_hat, mu, logvar, z = vae_model(image)\n",
    "\n",
    "        # Loss\n",
    "        loss = filtered_weighted_MSE(image_filtered, x_hat, mu, logvar)\n",
    "        #print(loss)\n",
    "\n",
    "        overall_loss += loss\n",
    "\n",
    "        # Update weights\n",
    "        \n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        \n",
    "        \n",
    "        time_iteration.append(time.time() - since_iter)\n",
    "        iter_time_mean = np.array(time_iteration).mean()\n",
    "        \n",
    "        if (i+1) % (5) == 0:\n",
    "            time_elapsed = time.time() - since\n",
    "            print(f\"Epoch: {epoch+1}/{num_epochs}, Step: {i+1}/{n_iterations}, Avg loss: {overall_loss/((i+1)*batch_size):.3f}, time: {time_elapsed:.2f}, Avg. per iter {iter_time_mean:.2f}, Est. time left {iter_time_mean*(n_iterations - (i+1)):.2f}\")\n",
    "            print()\n",
    "            \n",
    "        if (i+1) % (15) == 0:\n",
    "            imshow(image[0])\n",
    "            imshow(x_hat[0].detach())\n",
    "            \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        save_model_file_dir = save_model_file + f\"_{epoch+1}.pth\"\n",
    "        torch.save(vae_model.state_dict(), save_model_file_dir)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1e4c74",
   "metadata": {},
   "source": [
    "# Some results\n",
    "\n",
    "### BCE\n",
    "- 10 epochs, no batch norm: 60191.195\n",
    "\n",
    "### BCE with unpooling\n",
    "- 10 epochs, clear reconstruction but image indices carried through VAE. Faster training than BCE\n",
    "\n",
    "### ReLU + MSE loss\n",
    "- Avg loss: 2629.289, very poor reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d4a372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(vae_model.state_dict(), save_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc42698",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load_model_file = \"/home/patricknit/vae_models/vae_dronet_sigmoid_MSE_weighted_57.pth\"\n",
    "load_model=False\n",
    "\n",
    "if load_model:\n",
    "    vae_model.load_state_dict(torch.load(load_model_file))\n",
    "    vae_model.eval()\n",
    "else:\n",
    "    vae_model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58eeaf8",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73440ac3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vae_model.eval()\n",
    "\n",
    "# Visualise sample of images\n",
    "with torch.no_grad():\n",
    "    \n",
    "    images, *_ = next(iter(test_loader))\n",
    "\n",
    "    x_hat, *_ = vae_model.forward_test(images)\n",
    "\n",
    "    print(images.mean(), x_hat.mean())\n",
    "    print(images.var(), x_hat.var())\n",
    "    print(images.shape)\n",
    "    for idx in range(len(images)):\n",
    "        # show images\n",
    "        imshow(images[idx])\n",
    "        imshow(x_hat[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8acf03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset = DepthImageDataset(tfrecord_folder=tfrecord_folder, batch_size=batch_size)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=1, collate_fn=collate_batch)\n",
    "\n",
    "vae_model.eval()\n",
    "# Visualise sample of images\n",
    "\n",
    "with torch.no_grad():\n",
    "        \n",
    "    images, *_ = next(iter(train_loader)) # if dataset is shuffled every iter(), essentially test \n",
    "    # VAE forward pass\n",
    "    x_hat, *_ = vae_model.forward_test(images)\n",
    "\n",
    "    print(images.mean(), x_hat.mean())\n",
    "    print(images.var(), x_hat.var())\n",
    "    print(images.shape)\n",
    "    for idx in range(len(images)):\n",
    "        # show images\n",
    "        imshow(images[idx])\n",
    "        imshow(x_hat[idx])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
