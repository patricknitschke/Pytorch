{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e2cf0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-01 17:34:40.691655: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dde1be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8148d17",
   "metadata": {},
   "source": [
    "# Pytorch Dataset wrapper around tfrecords by Huan\n",
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8507c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_ending_with(folder_or_folders, ext):\n",
    "    if isinstance(folder_or_folders, str):\n",
    "        folder = folder_or_folders\n",
    "        assert os.path.exists(folder)\n",
    "\n",
    "        fnames = []\n",
    "        for fname in os.listdir(folder):\n",
    "            if fname.endswith(ext):       \n",
    "                fnames.append(os.path.join(folder, fname))\n",
    "        return sorted(fnames)\n",
    "    else:\n",
    "        assert hasattr(folder_or_folders, '__iter__')\n",
    "        print('folder_or_folders:', folder_or_folders)\n",
    "        return list(itertools.chain(*[get_files_ending_with(folder, ext) for folder in folder_or_folders]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5e702f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patricknit/anaconda3/envs/tf_gpu/lib/python3.9/site-packages/torch/cuda/__init__.py:80: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  ../c10/cuda/CUDAFunctions.cpp:112.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "def collate_batch(batch):\n",
    "    # image, actions, robot_state, collision_label, info_label, height, width, depth, action_horizon = [], [], [], [], [], [], [], [], []\n",
    "    # for _image, _actions, _robot_state, _collision_label, _info_label, _height, _width, _depth, _action_horizon in batch:\n",
    "    #     image.append(_image.numpy())\n",
    "    #     actions.append(_actions.numpy())\n",
    "    #     robot_state.append(_robot_state.numpy())\n",
    "    #     collision_label.append(_collision_label.numpy())\n",
    "    #     info_label.append(_info_label.numpy())\n",
    "    #     height.append(_height.numpy())\n",
    "    #     width.append(_width.numpy())\n",
    "    #     depth.append(_depth.numpy())\n",
    "    #     action_horizon.append(_action_horizon.numpy())\n",
    "    # print('batch[0]:', batch[0])\n",
    "    image = batch[0][0]\n",
    "    height = batch[0][1]\n",
    "    width = batch[0][2]\n",
    "    depth = batch[0][3]\n",
    "    \n",
    "    return torch.Tensor(np.array(image)).to(device), torch.Tensor(np.array(height)).to(device), torch.Tensor(np.array(width)).to(device), torch.Tensor(np.array(depth)).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1997df46",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "032c41ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthImageDataset(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, tfrecord_folder, batch_size=32, shuffle=True):\n",
    "        super(DepthImageDataset).__init__()\n",
    "        self.tfrecord_folder = tfrecord_folder\n",
    "        self.itr, self.itr_len = self.load_tfrecords(is_shuffle_and_repeat=shuffle, batch_size=batch_size)\n",
    "\n",
    "    def read_tfrecord(self, serialized_example):\n",
    "        feature_description = {\n",
    "            'image': tf.io.FixedLenFeature([], tf.string),\n",
    "            'height': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'width': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'depth': tf.io.FixedLenFeature([], tf.int64),\n",
    "        }\n",
    "        example = tf.io.parse_single_example(serialized_example, feature_description)\n",
    "\n",
    "        image = tf.transpose(tf.cast(tf.io.parse_tensor(example['image'], out_type = tf.uint8), tf.float32),  perm=[2, 0, 1]) / 256\n",
    "        height = example['height']\n",
    "        width = example['width']\n",
    "        depth = example['depth']\n",
    "        return image, height, width, depth\n",
    "\n",
    "    def load_tfrecords(self, is_shuffle_and_repeat=True, shuffle_buffer_size=5000, prefetch_buffer_size_multiplier=2, batch_size=32):\n",
    "        print('Loading tfrecords...')\n",
    "        tfrecord_fnames = get_files_ending_with(self.tfrecord_folder, '.tfrecords')\n",
    "        assert len(tfrecord_fnames) > 0\n",
    "        if is_shuffle_and_repeat:\n",
    "            np.random.shuffle(tfrecord_fnames)\n",
    "        else:\n",
    "            tfrecord_fnames = sorted(tfrecord_fnames) # 176 tfrecords for train, 20 for test\n",
    "\n",
    "        tfrecord_fnames = tfrecord_fnames[:1]\n",
    "        print(tfrecord_fnames)\n",
    "\n",
    "        dataset = tf.data.TFRecordDataset(tfrecord_fnames)\n",
    "        print(len([1 for _ in dataset]))\n",
    "        dataset = dataset.map(self.read_tfrecord, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "        if is_shuffle_and_repeat: \n",
    "            dataset = dataset.shuffle(buffer_size=shuffle_buffer_size, reshuffle_each_iteration=True)\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        dataset = dataset.prefetch(buffer_size=prefetch_buffer_size_multiplier * batch_size)\n",
    "        iterator = dataset.__iter__()\n",
    "        print('Done.')\n",
    "        \n",
    "        iter_len = sum(1 for _ in dataset)\n",
    "        \n",
    "        return iterator, iter_len\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self.itr\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.itr_len\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e75db1a",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "998d279b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tfrecords...\n",
      "['../../../rl_data/tfrecord/data87.tfrecords']\n",
      "1932\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method DepthImageDataset.read_tfrecord of <__main__.DepthImageDataset object at 0x7fcfcc1d1f70>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method DepthImageDataset.read_tfrecord of <__main__.DepthImageDataset object at 0x7fcfcc1d1f70>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "dataset = DepthImageDataset('../../../rl_data/tfrecord')\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=1, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc76501",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = iter(loader)\n",
    "data = data_iter.next()\n",
    "image = data[0]\n",
    "img_idx = 20\n",
    "io.imshow(image[img_idx,0,...].numpy())\n",
    "io.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6bb4b939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 1, 270, 480]),\n",
       " torch.Size([32]),\n",
       " torch.Size([32]),\n",
       " torch.Size([32]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image, height, width, depth = data\n",
    "image.shape, height.shape, width.shape, depth.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29724696",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 : torch.Size([32, 1, 270, 480])\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for image, height, width, depth in loader:\n",
    "    i += 1\n",
    "    if (i+1) % 50 == 0:\n",
    "        print(i, \":\", image.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
